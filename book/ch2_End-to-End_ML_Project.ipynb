{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rubber-immigration",
   "metadata": {},
   "source": [
    "# Chapter 2. End-to-End Machine Learning Project\n",
    "\n",
    "------------------------------------------------------\n",
    "In this chapter, you will go through an example project end to end, pretending to be a recently hired data scientist in a real estate company.1 Here are the main steps you will go through:\n",
    "\n",
    "    1. Look at the big picture.\n",
    "\n",
    "    2. Get the data.\n",
    "\n",
    "    3. Discover and visualize the data to gain insights.\n",
    "\n",
    "    4. Prepare the data for Machine Learning algorithms.\n",
    "\n",
    "    5. Select a model and train it.\n",
    "\n",
    "    6. Fine-tune your model.\n",
    "\n",
    "    7. Present your solution.\n",
    "\n",
    "    8. Launch, monitor, and maintain your system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-dominican",
   "metadata": {},
   "source": [
    "## Working with Real Data\n",
    "\n",
    "When you are learning about Machine Learning it is best to actually experiment with real-world data, not just artificial datasets. Fortunately, there are thousands of open datasets to choose from, ranging across all sorts of domains. Here are a few places you can look to get data:\n",
    "\n",
    "    Popular open data repositories:\n",
    "\n",
    "        * UC Irvine Machine Learning Repository\n",
    "\n",
    "        * Kaggle datasets\n",
    "\n",
    "        * Amazon’s AWS datasets\n",
    "\n",
    "    Meta portals (they list open data repositories):\n",
    "\n",
    "        * http://dataportals.org/\n",
    "\n",
    "        * http://opendatamonitor.eu/\n",
    "\n",
    "        * http://quandl.com/\n",
    "\n",
    "    Other pages listing many popular open data repositories:\n",
    "\n",
    "        * Wikipedia’s list of Machine Learning datasets\n",
    "\n",
    "        * Quora.com question\n",
    "\n",
    "        * Datasets subreddit\n",
    "\n",
    "In this chapter we chose the California Housing Prices dataset from the StatLib repository (see Figure 2-1). This dataset was based on data from the 1990 California census. It is not exactly recent (you could still afford a nice house in the Bay Area at the time), but it has many qualities for learning, so we will pretend it is recent data. We also added a categorical attribute and removed a few features for teaching purposes.\n",
    "\n",
    "![Figure 2-1. California housing prices](img/2-1_CA_housing_prices.png)\n",
    "\n",
    "Figure 2-1. California housing prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-canadian",
   "metadata": {},
   "source": [
    "## Look at the Big Picture\n",
    "\n",
    "Welcome to Machine Learning Housing Corporation! The first task you are asked to perform is to build a model of housing prices in California using the California census data. This data has metrics such as the population, median income, median housing price, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them “districts” for short.\n",
    "\n",
    "Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.\n",
    "\n",
    "---------------------\n",
    "\n",
    "### Tip\n",
    "\n",
    "_Since you are a well-organized data scientist, the first thing you do is to pull out your Machine Learning project checklist. You can start with the one in Appendix B; it should work reasonably well for most Machine Learning projects but make sure to adapt it to your needs. In this chapter we will go through many checklist items, but we will also skip a few, either because they are self-explanatory or because they will be discussed in later chapters._\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-flower",
   "metadata": {},
   "source": [
    "## Frame the Problem\n",
    "\n",
    "The first question to ask your boss is what exactly is the business objective; building a model is probably not the end goal. How does the company expect to use and benefit from this model? This is important because it will determine how you frame the problem, what algorithms you will select, what performance measure you will use to evaluate your model, and how much effort you should spend tweaking it.\n",
    "\n",
    "Your boss answers that your model’s output (a prediction of a district’s median housing price) will be fed to another Machine Learning system (see Figure 2-2), along with many other signals.3 This downstream system will determine whether it is worth investing in a given area or not. Getting this right is critical, as it directly affects revenue.\n",
    "!['Figure 2-2. A Machine Learning pipeline for real estate investments'](img/2-2_A_ML_pipeline.png)\n",
    "\n",
    "Figure 2-2. A Machine Learning pipeline for real estate investments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-failure",
   "metadata": {},
   "source": [
    "------------------\n",
    "### Pipelines\n",
    "\n",
    "_A sequence of data processing components is called a data pipeline. Pipelines are very common in Machine Learning systems, since there is a lot of data to manipulate and many data transformations to apply._\n",
    "\n",
    "_Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store, and then some time later the next component in the pipeline pulls this data and spits out its own output, and so on. Each component is fairly self-contained: the interface between components is simply the data store. This makes the system quite simple to grasp (with the help of a data flow graph), and different teams can focus on different components. Moreover, if a component breaks down, the downstream components can often continue to run normally (at least for a while) by just using the last output from the broken component. This makes the architecture quite robust._\n",
    "\n",
    "_On the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overall system’s performance drops._\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-listing",
   "metadata": {},
   "source": [
    "The next question to ask is what the current solution looks like (if any). It will often give you a reference performance, as well as insights on how to solve the problem. Your boss answers that the district housing prices are currently estimated manually by experts: a team gathers up-to-date information about a district, and when they cannot get the median housing price, they estimate it using complex rules.\n",
    "\n",
    "This is costly and time-consuming, and their estimates are not great; in cases where they manage to find out the actual median housing price, they often realize that their estimates were off by more than 20%. This is why the company thinks that it would be useful to train a model to predict a district’s median housing price given other data about that district. The census data looks like a great dataset to exploit for this purpose, since it includes the median housing prices of thousands of districts, as well as other data.\n",
    "\n",
    "Okay, with all this information you are now ready to start designing your system. First, you need to frame the problem: is it supervised, unsupervised, or Reinforcement Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.\n",
    "\n",
    "Have you found the answers? Let’s see: it is clearly a typical supervised learning task since you are given labeled training examples (each instance comes with the expected output, i.e., the district’s median housing price). Moreover, it is also a typical regression task, since you are asked to predict a value. More specifically, this is a multiple regression problem since the system will use multiple features to make a prediction (it will use the district’s population, the median income, etc.). It is also a univariate regression problem since we are only trying to predict a single value for each district. If we were trying to predict multiple values per district, it would be a multivariate regression problem. Finally, there is no continuous flow of data coming in the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine.\n",
    "\n",
    "-------------\n",
    "Tip\n",
    "\n",
    "If the data was huge, you could either split your batch learning work across multiple servers (using the MapReduce technique), or you could use an online learning technique instead.\n",
    "\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-thursday",
   "metadata": {},
   "source": [
    "#### Select a Performance Measure\n",
    "\n",
    "Your next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors. Equation 2-1 shows the mathematical formula to compute the RMSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-cooler",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-venture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
